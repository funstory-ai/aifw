name: aifw-release

on:
  release:
    types: [created]

jobs:
  docker:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up QEMU
        uses: docker/setup-qemu-action@v3

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Login to GHCR
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.repository_owner }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Extract metadata
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ghcr.io/${{ github.repository_owner }}/oneaifw
          tags: |
            type=ref,event=tag
            type=raw,value=latest

      - name: Build and push (multi-arch)
        uses: docker/build-push-action@v6
        with:
          context: py-origin
          file: Dockerfile
          push: true
          platforms: linux/amd64,linux/arm64
          build-args: |
            SPACY_PROFILE=minimal
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}

      - name: Build and push (web)
        uses: docker/build-push-action@v6
        with:
          context: .
          file: web/Dockerfile
          push: true
          platforms: linux/amd64
          build-args: |
            SPACY_PROFILE=minimal
          tags: |
            ghcr.io/${{ github.repository_owner }}/oneaifw-web:${{ github.ref_name }}
            ghcr.io/${{ github.repository_owner }}/oneaifw-web:latest
          labels: |
            org.opencontainers.image.title=OneAIFW Web
            org.opencontainers.image.description=AI Framework Web Interface
            org.opencontainers.image.version=${{ github.ref_name }}
            org.opencontainers.image.created=${{ github.event.repository.updated_at }}
            org.opencontainers.image.source=${{ github.server_url }}/${{ github.repository }}

      - name: Start fake LLM (echo) on runner
        run: |
          cd py-origin
          python -m pip install --upgrade pip
          pip install fastapi uvicorn pydantic
          python -m uvicorn services.fake_llm.echo_server:app --host 0.0.0.0 --port 8801 &
          echo $! > echo_llm.pid
          for i in $(seq 1 20); do curl -sf http://127.0.0.1:8801/v1/health && break || sleep 0.5; done

      - name: Prepare OpenAI-compatible docker key file (host.docker.internal)
        run: |
          # This file points to the fake LLM running on the runner host.
          # Inside Docker, 127.0.0.1 refers to the container itself, so we use
          # host.docker.internal (mapped via --add-host below) to reach the host.
          cat > $RUNNER_TEMP/echo-apikey-docker.json << 'JSON'
          {
            "openai-api-key": "test-local-echo",
            "openai-base-url": "http://host.docker.internal:8801/v1",
            "openai-model": "echo-001"
          }
          JSON

      - name: Smoke test container (launch → call → stop)
        run: |
          PROMPT="请把如下文本翻译为中文: My email address is test@example.com, and my phone number is 18744325579."
          # Map host.docker.internal to host gateway so the container can reach the fake LLM on the runner
          docker run --rm \
            --add-host=host.docker.internal:host-gateway \
            -e AIFW_API_KEY_FILE=/data/aifw/echo.json \
            -v $RUNNER_TEMP/echo-apikey-docker.json:/data/aifw/echo.json \
            ghcr.io/${{ github.repository_owner }}/oneaifw:latest \
            sh -lc '
              python -m aifw launch &
              for i in $(seq 1 40); do curl -sf http://127.0.0.1:8844/api/health && break || sleep 0.5; done
              python -m aifw call "$PROMPT"
              python -m aifw stop'

      - name: Smoke test container (direct_call)
        run: |
          PROMPT="请把如下文本翻译为中文: My email address is test@example.com, and my phone number is 18744325579."
          # Map host.docker.internal to host gateway so the container can reach the fake LLM on the runner
          docker run --rm \
            --add-host=host.docker.internal:host-gateway \
            -e AIFW_API_KEY_FILE=/data/aifw/echo.json \
            -v $RUNNER_TEMP/echo-apikey-docker.json:/data/aifw/echo.json \
            ghcr.io/${{ github.repository_owner }}/oneaifw:latest \
            sh -lc 'python -m aifw direct_call "$PROMPT"'

      - name: Teardown fake LLM
        if: always()
        run: |
          if [ -f echo_llm.pid ]; then kill $(cat echo_llm.pid) || true; fi

