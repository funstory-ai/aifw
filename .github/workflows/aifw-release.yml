name: aifw-release

on:
  release:
    types: [created]

jobs:
  docker:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up QEMU
        uses: docker/setup-qemu-action@v3

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Login to GHCR
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.repository_owner }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Extract metadata
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ghcr.io/${{ github.repository_owner }}/oneaifw
          tags: |
            type=ref,event=tag
            type=raw,value=latest

      - name: Build and push (multi-arch)
        uses: docker/build-push-action@v6
        with:
          context: .
          push: true
          platforms: linux/amd64,linux/arm64
          build-args: |
            SPACY_PROFILE=minimal
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}

      - name: Start fake LLM (echo) on runner
        run: |
          python -m pip install --upgrade pip
          pip install fastapi uvicorn pydantic
          python -m uvicorn services.fake_llm.echo_server:app --host 127.0.0.1 --port 8801 &
          echo $! > echo_llm.pid
          for i in $(seq 1 20); do curl -sf http://127.0.0.1:8801/v1/health && break || sleep 0.5; done

      - name: Prepare OpenAI-compatible docker key file (host.docker.internal)
        run: |
          # This file points to the fake LLM running on the runner host.
          # Inside Docker, 127.0.0.1 refers to the container itself, so we use
          # host.docker.internal (mapped via --add-host below) to reach the host.
          cat > $RUNNER_TEMP/echo-apikey-docker.json << 'JSON'
          {
            "openai-api-key": "test-local-echo",
            "openai-base-url": "http://host.docker.internal:8801/v1",
            "openai-model": "echo-001"
          }
          JSON

      - name: Smoke test container (launch → call → stop)
        run: |
          PROMPT="请把如下文本翻译为中文: My email address is test@example.com, and my phone number is 18744325579."
          # map special DNS name to host gateway, so that the container can reach the host.
          docker run --rm \
            --add-host=host.docker.internal:host-gateway \
            -e AIFW_API_KEY_FILE=/data/aifw/echo.json \
            -v $RUNNER_TEMP/echo-apikey-docker.json:/data/aifw/echo.json \
            ghcr.io/${{ github.repository_owner }}/oneaifw:latest \
            sh -lc '
              python -m aifw launch &
              for i in $(seq 1 40); do curl -sf http://127.0.0.1:8844/api/health && break || sleep 0.5; done
              python -m aifw call "$PROMPT"
              python -m aifw stop'

      - name: Smoke test container (direct_call)
        run: |
          PROMPT="请把如下文本翻译为中文: My email address is test@example.com, and my phone number is 18744325579."
          # map special DNS name to host gateway, so that the container can reach the host.
          docker run --rm \
            --add-host=host.docker.internal:host-gateway \
            -e AIFW_API_KEY_FILE=/data/aifw/echo.json \
            -v $RUNNER_TEMP/echo-apikey-docker.json:/data/aifw/echo.json \
            ghcr.io/${{ github.repository_owner }}/oneaifw:latest \
            sh -lc 'python -m aifw direct_call "$PROMPT"'

      - name: Teardown fake LLM
        if: always()
        run: |
          if [ -f echo_llm.pid ]; then kill $(cat echo_llm.pid) || true; fi

